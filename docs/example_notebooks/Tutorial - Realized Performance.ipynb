{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b002ad14",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nannyml as nml\n",
    "from IPython.display import display\n",
    "reference, analysis, analysis_target = nml.datasets.load_synthetic_binary_classification_dataset()\n",
    "display(reference.head(3))\n",
    "\n",
    "data = pd.concat([reference, analysis.set_index('identifier').join(analysis_target.set_index('identifier'), on='identifier', rsuffix='_r')], ignore_index=True).reset_index(drop=True)\n",
    "display(data.loc[data['partition'] == 'analysis'].head(3))\n",
    "\n",
    "metadata = nml.extract_metadata(reference, model_type=nml.ModelType.CLASSIFICATION_BINARY, exclude_columns=['identifier'])\n",
    "metadata.target_column_name = 'work_home_actual'\n",
    "display(metadata.is_complete())\n",
    "\n",
    "performance_calculator = nml.PerformanceCalculator(\n",
    "    model_metadata=metadata,\n",
    "    # use NannyML to tell us what metrics are supported\n",
    "    metrics=nml.performance_estimation.confidence_based.results.SUPPORTED_METRIC_VALUES,\n",
    "    chunk_size=5000\n",
    ").fit(reference_data=reference)\n",
    "\n",
    "realized_performance = performance_calculator.calculate(data)\n",
    "\n",
    "display(realized_performance.data.head(3))\n",
    "\n",
    "for metric in performance_calculator.metrics:\n",
    "    realized_performance.plot(kind='performance', metric=metric).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6852d26c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nannyml as nml\n",
    "from IPython.display import display\n",
    "\n",
    "reference, analysis, analysis_target = nml.datasets.load_synthetic_binary_classification_dataset()\n",
    "display(reference.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eed134",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(reference.head(3).to_markdown(tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e576fa4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.concat([reference, analysis.set_index('identifier').join(analysis_target.set_index('identifier'), on='identifier', rsuffix='_r')], ignore_index=True).reset_index(drop=True)\n",
    "display(data.loc[data['partition'] == 'analysis'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce717d15",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data.loc[data['partition'] == 'analysis'].head(3).to_markdown(tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b9d9f4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata = nml.extract_metadata(reference, model_type=nml.ModelType.CLASSIFICATION_BINARY, exclude_columns=['identifier'])\n",
    "metadata.target_column_name = 'work_home_actual'\n",
    "display(metadata.is_complete())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadf9479",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(metadata.to_df().to_markdown(tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a5dc3b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "performance_calculator = nml.PerformanceCalculator(\n",
    "    model_metadata=metadata,\n",
    "    # use NannyML to tell us what metrics are supported\n",
    "    metrics=nml.performance_estimation.confidence_based.results.SUPPORTED_METRIC_VALUES,\n",
    "    chunk_size=5000\n",
    ").fit(reference_data=reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856b0ee2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "realized_performance = performance_calculator.calculate(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bc8fd2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "display(realized_performance.data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261cb5e8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(realized_performance.data.head(3).to_markdown(tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a210eee",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for metric in performance_calculator.metrics:\n",
    "    figure = realized_performance.plot(kind='performance', metric=metric)\n",
    "    figure.show()\n",
    "    # save figure - not shown on guide:\n",
    "    # print(metric.display_name.replace(\" \", \"_\"))\n",
    "    # figure.write_image(file=f\"../_static/tutorial-perf-guide-{metric.display_name.replace(' ', '_')}.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10541a5d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c206974c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7c7d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nannyml as nml\n",
    "from IPython.display import display\n",
    "reference, analysis, analysis_gt = nml.datasets.load_synthetic_multiclass_classification_dataset()\n",
    "display(reference.head(3))\n",
    "\n",
    "data = pd.concat([\n",
    "    reference,\n",
    "    analysis.set_index('identifier').join(analysis_gt.set_index('identifier'), on='identifier', rsuffix='_r')\n",
    "], ignore_index=True).reset_index(drop=True)\n",
    "display(data.loc[data['partition'] == 'analysis'].head(3))\n",
    "\n",
    "metadata = nml.extract_metadata(\n",
    "    reference,\n",
    "    model_name='credit_card_segment',\n",
    "    model_type=nml.ModelType.CLASSIFICATION_MULTICLASS,\n",
    "    exclude_columns=['identifier']\n",
    ")\n",
    "metadata.target_column_name = 'y_true'\n",
    "display(metadata.is_complete())\n",
    "\n",
    "performance_calculator = nml.PerformanceCalculator(\n",
    "    model_metadata=metadata,\n",
    "    metrics=['roc_auc', 'f1'],\n",
    "    chunk_size=6000\n",
    ").fit(reference_data=reference)\n",
    "\n",
    "realized_performance = performance_calculator.calculate(data)\n",
    "\n",
    "display(realized_performance.data.head(3))\n",
    "\n",
    "for metric in performance_calculator.metrics:\n",
    "    figure = realized_performance.plot(kind='performance', metric=metric)\n",
    "    figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd20e97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nannyml as nml\n",
    "from IPython.display import display\n",
    "reference, analysis, analysis_gt = nml.datasets.load_synthetic_multiclass_classification_dataset()\n",
    "display(reference.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a3dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reference.head(3).to_markdown(tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19610cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([\n",
    "    reference,\n",
    "    analysis.set_index('identifier').join(analysis_gt.set_index('identifier'), on='identifier', rsuffix='_r')\n",
    "], ignore_index=True).reset_index(drop=True)\n",
    "display(data.loc[data['partition'] == 'analysis'].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5343d22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.loc[data['partition'] == 'analysis'].head(3).to_markdown(tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20416388",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = nml.extract_metadata(\n",
    "    reference,\n",
    "    model_name='credit_card_segment',\n",
    "    model_type=nml.ModelType.CLASSIFICATION_MULTICLASS,\n",
    "    exclude_columns=['identifier']\n",
    ")\n",
    "metadata.target_column_name = 'y_true'\n",
    "display(metadata.is_complete())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50be7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_calculator = nml.PerformanceCalculator(\n",
    "    model_metadata=metadata,\n",
    "    metrics=['roc_auc', 'f1'],\n",
    "    chunk_size=6000\n",
    ").fit(reference_data=reference)\n",
    "\n",
    "realized_performance = performance_calculator.calculate(data)\n",
    "\n",
    "display(realized_performance.data.head(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333cbcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(realized_performance.data.head(3).to_markdown(tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a40e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in performance_calculator.metrics:\n",
    "    figure = realized_performance.plot(kind='performance', metric=metric)\n",
    "    figure.show()\n",
    "    # save figure - not shown on guide:\n",
    "    print(metric.display_name.replace(\" \", \"_\"))\n",
    "    figure.write_image(file=f\"../_static/tutorial-perf-guide-mc-{metric.display_name.replace(' ', '_')}.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7fab8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
